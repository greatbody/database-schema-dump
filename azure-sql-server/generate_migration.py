#!/usr/bin/env python3
"""
Generate a re-runnable SQL migration script for Azure SQL Server programmable objects.

This script connects using credentials from environment variables (via .env) and
collects definitions for:
  - Schemas (user-defined)
    - Tables (definitions only, no data)
  - User-defined types (alias + table types)
  - Sequences
  - Synonyms
  - Views, Stored Procedures, Functions, Triggers

It outputs migration_script.sql with only CREATE statements guarded by IF NOT EXISTS checks.
No DROP or ALTER is emitted to keep it production-safe and re-runnable.
"""

from __future__ import annotations

import os
import re
import sys
import textwrap
from typing import Dict, Iterable, List, Optional, Tuple

import pyodbc  # type: ignore
from dotenv import load_dotenv  # type: ignore


# -------------------------
# Configuration & Utilities
# -------------------------

DEFAULT_DRIVER = os.environ.get("ODBC_DRIVER", "ODBC Driver 18 for SQL Server")


def load_config() -> Dict[str, str]:
    """Load configuration from .env and environment variables."""
    load_dotenv(override=False)

    cfg = {
        "SQLSERVER_HOST": os.environ.get("SQLSERVER_HOST", ""),
        "SQLSERVER_DATABASE": os.environ.get("SQLSERVER_DATABASE", ""),
        "SQLSERVER_USERNAME": os.environ.get("SQLSERVER_USERNAME", ""),
        "SQLSERVER_PASSWORD": os.environ.get("SQLSERVER_PASSWORD", ""),
        "ODBC_DRIVER": os.environ.get("ODBC_DRIVER", DEFAULT_DRIVER),
        "ENCRYPT": os.environ.get("SQLSERVER_ENCRYPT", "yes"),
        "TRUST_SERVER_CERT": os.environ.get("SQLSERVER_TRUST_SERVER_CERT", "no"),
        "CONNECTION_TIMEOUT": os.environ.get("SQLSERVER_CONNECTION_TIMEOUT", "30"),
    }

    missing = [k for k in ("SQLSERVER_HOST", "SQLSERVER_DATABASE", "SQLSERVER_USERNAME", "SQLSERVER_PASSWORD") if not cfg.get(k)]
    if missing:
        raise RuntimeError(
            "Missing required environment variables: " + ", ".join(missing) + "\n"
            "Create a .env file (see .env.example) and set these values."
        )

    return cfg


def connect(cfg: Dict[str, str]) -> pyodbc.Connection:
    """Create a pyodbc connection to Azure SQL Server using the provided config."""
    # Azure SQL often requires Encrypt=yes; TrustServerCertificate=no
    # SERVER can be host or tcp:host,1433
    server = cfg["SQLSERVER_HOST"]
    database = cfg["SQLSERVER_DATABASE"]
    username = cfg["SQLSERVER_USERNAME"]
    password = cfg["SQLSERVER_PASSWORD"]
    driver = cfg["ODBC_DRIVER"]
    encrypt = cfg["ENCRYPT"]
    trust = cfg["TRUST_SERVER_CERT"]
    timeout = cfg["CONNECTION_TIMEOUT"]

    conn_str = (
        f"DRIVER={{{driver}}};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={username};"
        f"PWD={password};"
        f";Encrypt={encrypt};TrustServerCertificate={trust};Connection Timeout={timeout}"
    )

    return pyodbc.connect(conn_str)


def br(name: str) -> str:
    """Bracket-quote an identifier."""
    # Escape any closing bracket in the name
    safe = name.replace("]", "]]")
    return f"[{safe}]"


def qual(schema: str, name: str) -> str:
    return f"{br(schema)}.{br(name)}"


def tsql_quote(s: str) -> str:
    """Escape a string for inclusion in a T-SQL string literal."""
    return s.replace("'", "''")


def normalize_create(defn: str) -> str:
    """Ensure the module definition starts with CREATE, not ALTER."""
    # Replace only the first statement keyword if it's ALTER <OBJECT>
    pattern = re.compile(r"^\s*ALTER(\s+\w+)\b", re.IGNORECASE)
    return pattern.sub(lambda m: "CREATE" + m.group(1), defn, count=1)


def write_header(out) -> None:
    out.write("-- Generated by generate_migration.py\n")
    out.write("-- Safe, re-runnable creation of programmable objects (no DROP/ALTER)\n")
    out.write("-- Timestamp (UTC): ")
    import datetime as _dt

    # Use timezone-aware UTC to avoid deprecation warnings
    out.write(_dt.datetime.now(_dt.timezone.utc).isoformat() + "\n")
    out.write("GO\n\n")


# -------------------------
# Fetchers & Formatters
# -------------------------


def fetch_schemas(cur) -> List[str]:
    cur.execute(
        """
        SELECT s.name
        FROM sys.schemas AS s
        WHERE s.name NOT IN ('sys','INFORMATION_SCHEMA','dbo')
        ORDER BY s.name
        """
    )
    return [row[0] for row in cur.fetchall()]


def generate_schema_sql(schemas: Iterable[str]) -> List[str]:
    blocks: List[str] = []
    for s in schemas:
        stmt = (
            "IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = N'" + tsql_quote(s) + "')\n"
            "    EXEC(N'CREATE SCHEMA "
            + br(s)
            + "');\nGO\n"
        )
        blocks.append(stmt)
    return blocks


def format_system_type(typ: str, max_length: int, precision: int, scale: int) -> str:
    """Format a system type with length/precision/scale as needed."""
    t = typ.lower()
    # Types with (length)
    if t in {"binary", "varbinary", "char", "varchar", "nchar", "nvarchar"}:
        if max_length == -1:
            size = "max"
        else:
            # nvarchar/nchar length is in bytes; divide by 2
            if t in {"nchar", "nvarchar"}:
                size = str(max_length // 2)
            else:
                size = str(max_length)
        return f"{typ}({size})"
    # Types with (precision, scale)
    if t in {"decimal", "numeric"}:
        return f"{typ}({precision},{scale})"
    # Types with (scale)
    if t in {"time", "datetime2", "datetimeoffset"}:
        return f"{typ}({scale})"
    # float with precision is rarely required; omit for simplicity
    return typ


def fetch_alias_types(cur) -> List[Dict[str, object]]:
    cur.execute(
        """
        SELECT s.name AS schema_name,
               t.name AS type_name,
               bt.name AS base_type,
               t.max_length,
               t.precision,
               t.scale,
               t.is_nullable
        FROM sys.types AS t
        JOIN sys.schemas AS s ON t.schema_id = s.schema_id
        JOIN sys.types   AS bt ON t.system_type_id = bt.system_type_id AND bt.user_type_id = bt.system_type_id
        WHERE t.is_user_defined = 1 AND t.is_assembly_type = 0 AND t.is_table_type = 0
        ORDER BY s.name, t.name
        """
    )
    rows = []
    for r in cur.fetchall():
        rows.append(
            {
                "schema": r[0],
                "name": r[1],
                "base_type": r[2],
                "max_length": int(r[3]),
                "precision": int(r[4]),
                "scale": int(r[5]),
                "is_nullable": bool(r[6]),
            }
        )
    return rows


def generate_alias_type_sql(rows: List[Dict[str, object]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name = str(r["schema"]), str(r["name"])
        base = format_system_type(str(r["base_type"]), int(r["max_length"]), int(r["precision"]), int(r["scale"]))
        nullability = " NULL" if bool(r["is_nullable"]) else " NOT NULL"
        create_stmt = f"CREATE TYPE {qual(schema, name)} FROM {base}{nullability};"
        block = (
            f"IF TYPE_ID(N'{tsql_quote(schema)}.{tsql_quote(name)}') IS NULL\n"
            f"    EXEC(N'{tsql_quote(create_stmt)}');\nGO\n"
        )
        blocks.append(block)
    return blocks


def fetch_table_types(cur) -> List[Dict[str, object]]:
    cur.execute(
        """
        SELECT tt.user_type_id,
               s.name AS schema_name,
               tt.name AS type_name,
               tt.type_table_object_id
        FROM sys.table_types AS tt
        JOIN sys.schemas AS s ON tt.schema_id = s.schema_id
        WHERE tt.is_user_defined = 1
        ORDER BY s.name, tt.name
        """
    )
    results = []
    for user_type_id, schema_name, type_name, obj_id in cur.fetchall():
        # Columns
        cur.execute(
            """
            SELECT c.name,
                   bt.name AS system_type,
                   c.max_length,
                   c.precision,
                   c.scale,
                   c.is_nullable,
                   c.is_identity
            FROM sys.columns AS c
            JOIN sys.types AS bt
              ON c.system_type_id = bt.system_type_id AND bt.user_type_id = bt.system_type_id
            WHERE c.object_id = ?
            ORDER BY c.column_id
            """,
            obj_id,
        )
        columns = [
            {
                "name": row[0],
                "system_type": row[1],
                "max_length": int(row[2]),
                "precision": int(row[3]),
                "scale": int(row[4]),
                "is_nullable": bool(row[5]),
                "is_identity": bool(row[6]),
            }
            for row in cur.fetchall()
        ]

        # Identity details per column (if any)
        cur.execute(
            """
         -- Cast sql_variant values to BIGINT to avoid unsupported ODBC types
         SELECT c.name,
             CAST(ic.seed_value AS BIGINT)      AS seed_value,
             CAST(ic.increment_value AS BIGINT) AS increment_value
            FROM sys.identity_columns AS ic
            JOIN sys.columns AS c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
            WHERE ic.object_id = ?
            """,
            obj_id,
        )
        identity_map: Dict[str, Tuple[int, int]] = {row[0]: (int(row[1]), int(row[2])) for row in cur.fetchall()}

        # Default constraints per column
        cur.execute(
            """
         -- Avoid NVARCHAR(MAX) (WLONGVARCHAR) driver issues by narrowing to NVARCHAR(4000)
         SELECT c.name AS column_name,
             CAST(dc.definition AS NVARCHAR(4000)) AS definition
            FROM sys.default_constraints AS dc
            JOIN sys.columns AS c ON dc.parent_object_id = c.object_id AND dc.parent_column_id = c.column_id
            WHERE dc.parent_object_id = ?
            """,
            obj_id,
        )
        default_map: Dict[str, str] = {row[0]: str(row[1]) for row in cur.fetchall()}

        # Primary key (optional)
        cur.execute(
            """
            SELECT i.index_id, i.name, i.type_desc
            FROM sys.indexes AS i
            WHERE i.object_id = ? AND i.is_primary_key = 1
            """,
            obj_id,
        )
        pk_row = cur.fetchone()
        pk: Optional[Dict[str, object]] = None
        if pk_row:
            index_id, pk_name, type_desc = int(pk_row[0]), str(pk_row[1] or f"PK_{type_name}"), str(pk_row[2])
            # PK columns
            cur.execute(
                """
                SELECT c.name, ic.is_descending_key
                FROM sys.index_columns AS ic
                JOIN sys.columns AS c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
                WHERE ic.object_id = ? AND ic.index_id = ?
                ORDER BY ic.key_ordinal
                """,
                obj_id,
                index_id,
            )
            pk_cols = [(str(r[0]), bool(r[1])) for r in cur.fetchall()]
            pk = {"name": pk_name, "type_desc": type_desc, "columns": pk_cols}

        results.append(
            {
                "schema": schema_name,
                "name": type_name,
                "columns": columns,
                "identity_map": identity_map,
                "default_map": default_map,
                "pk": pk,
            }
        )
    return results


def generate_table_type_sql(rows: List[Dict[str, object]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name = str(r["schema"]), str(r["name"])
        columns: List[Dict[str, object]] = list(r["columns"])  # type: ignore
        identity_map: Dict[str, Tuple[int, int]] = dict(r["identity_map"])  # type: ignore
        default_map: Dict[str, str] = dict(r["default_map"])  # type: ignore
        pk: Optional[Dict[str, object]] = r.get("pk")  # type: ignore

        col_defs: List[str] = []
        for c in columns:
            cname = str(c["name"])
            ctype = format_system_type(str(c["system_type"]), int(c["max_length"]), int(c["precision"]), int(c["scale"]))
            nullability = " NULL" if bool(c["is_nullable"]) else " NOT NULL"
            identity = ""
            if cname in identity_map:
                seed, inc = identity_map[cname]
                identity = f" IDENTITY({seed},{inc})"
            default = ""
            if cname in default_map:
                # default_map definition already includes surrounding parentheses
                default = f" DEFAULT {default_map[cname]}"
            col_defs.append(f"{br(cname)} {ctype}{identity}{default}{nullability}")

        # Primary key constraint
        if pk:
            pk_name = br(str(pk["name"]))
            clustered = "CLUSTERED" if "CLUSTERED" in str(pk["type_desc"]).upper() else "NONCLUSTERED"
            col_list = ", ".join([f"{br(col)} {'DESC' if desc else 'ASC'}" for col, desc in pk["columns"]])  # type: ignore
            col_defs.append(f"CONSTRAINT {pk_name} PRIMARY KEY {clustered} ({col_list})")

        create_body = ",\n    ".join(col_defs)
        create_stmt = f"CREATE TYPE {qual(schema, name)} AS TABLE (\n    {create_body}\n);"
        block = (
            f"IF TYPE_ID(N'{tsql_quote(schema)}.{tsql_quote(name)}') IS NULL\n"
            f"    EXEC(N'{tsql_quote(create_stmt)}');\nGO\n"
        )
        blocks.append(block)
    return blocks


def fetch_sequences(cur) -> List[Dict[str, object]]:
    cur.execute(
        """
    SELECT sch.name AS schema_name,
           seq.name AS sequence_name,
           bt.name  AS base_type,
           seq.precision,
           seq.scale,
           CAST(seq.start_value    AS BIGINT) AS start_value,
           CAST(seq.increment      AS BIGINT) AS increment,
           CAST(seq.minimum_value  AS BIGINT) AS minimum_value,
           CAST(seq.maximum_value  AS BIGINT) AS maximum_value,
           seq.is_cycling,
           seq.cache_size
        FROM sys.sequences AS seq
        JOIN sys.schemas AS sch ON seq.schema_id = sch.schema_id
        JOIN sys.types   AS bt  ON seq.system_type_id = bt.system_type_id AND bt.user_type_id = bt.system_type_id
        ORDER BY sch.name, seq.name
        """
    )
    seqs = []
    for r in cur.fetchall():
        seqs.append(
            {
                "schema": r[0],
                "name": r[1],
                "base_type": r[2],
                "precision": int(r[3]),
                "scale": int(r[4]),
                "start_value": int(r[5]),
                "increment": int(r[6]),
                "min_value": int(r[7]),
                "max_value": int(r[8]),
                "is_cycling": bool(r[9]),
                "cache_size": int(r[10]),
            }
        )
    return seqs


def generate_sequence_sql(rows: List[Dict[str, object]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name = str(r["schema"]), str(r["name"])
        base = str(r["base_type"]).lower()
        # Include precision/scale for decimal/numeric sequences if present
        if base in {"decimal", "numeric"}:
            base_type = f"{r['base_type']}({r['precision']},{r['scale']})"
        else:
            base_type = str(r["base_type"])  # e.g., bigint
        cycle = "CYCLE" if r["is_cycling"] else "NO CYCLE"
        cache = f"CACHE {r['cache_size']}" if int(r["cache_size"]) > 0 else "NO CACHE"
        create_stmt = (
            f"CREATE SEQUENCE {qual(schema, name)} AS {base_type} \n"
            f"    START WITH {r['start_value']} \n"
            f"    INCREMENT BY {r['increment']} \n"
            f"    MINVALUE {r['min_value']} MAXVALUE {r['max_value']} \n"
            f"    {cycle} \n"
            f"    {cache};"
        )
        block = (
            f"IF NOT EXISTS (SELECT 1 FROM sys.sequences WHERE name = N'{tsql_quote(name)}' AND schema_id = SCHEMA_ID(N'{tsql_quote(schema)}'))\n"
            f"    {create_stmt}\nGO\n"
        )
        blocks.append(block)
    return blocks


def fetch_synonyms(cur) -> List[Dict[str, str]]:
    cur.execute(
        """
        SELECT s.name AS schema_name, sy.name AS synonym_name, sy.base_object_name
        FROM sys.synonyms AS sy
        JOIN sys.schemas AS s ON sy.schema_id = s.schema_id
        ORDER BY s.name, sy.name
        """
    )
    return [{"schema": r[0], "name": r[1], "base": r[2]} for r in cur.fetchall()]


def generate_synonym_sql(rows: List[Dict[str, str]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name, base = r["schema"], r["name"], r["base"]
        block = (
            f"IF OBJECT_ID(N'{tsql_quote(schema)}.{tsql_quote(name)}', 'SN') IS NULL\n"
            f"    CREATE SYNONYM {qual(schema, name)} FOR {base};\nGO\n"
        )
        blocks.append(block)
    return blocks

def fetch_tables(cur) -> List[Dict[str, object]]:
    """Fetch user tables with column metadata, defaults, identities, and PKs.
    Avoid unsupported ODBC types by casting sql_variant and NVARCHAR(MAX) to safe types.
    """
    cur.execute(
        """
        SELECT t.object_id,
               s.name AS schema_name,
               t.name AS table_name
        FROM sys.tables AS t
        JOIN sys.schemas AS s ON t.schema_id = s.schema_id
        WHERE t.is_ms_shipped = 0
        ORDER BY s.name, t.name
        """
    )
    tables: List[Dict[str, object]] = []
    for obj_id, schema_name, table_name in cur.fetchall():
        # Columns (basic attributes)
        cur.execute(
            """
            SELECT c.name,
                   bt.name AS system_type,
                   c.max_length,
                   c.precision,
                   c.scale,
                   c.is_nullable,
                   c.is_identity,
                   c.is_computed,
                   c.collation_name
            FROM sys.columns AS c
            JOIN sys.types   AS bt
              ON c.system_type_id = bt.system_type_id AND bt.user_type_id = bt.system_type_id
            WHERE c.object_id = ?
            ORDER BY c.column_id
            """,
            obj_id,
        )
        columns = [
            {
                "name": r[0],
                "system_type": r[1],
                "max_length": int(r[2]),
                "precision": int(r[3]),
                "scale": int(r[4]),
                "is_nullable": bool(r[5]),
                "is_identity": bool(r[6]),
                "is_computed": bool(r[7]),
                "collation": (str(r[8]) if r[8] is not None else None),
            }
            for r in cur.fetchall()
        ]

        # Identity details per column
        cur.execute(
            """
            SELECT c.name,
                   CAST(ic.seed_value AS BIGINT)      AS seed_value,
                   CAST(ic.increment_value AS BIGINT) AS increment_value
            FROM sys.identity_columns AS ic
            JOIN sys.columns AS c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
            WHERE ic.object_id = ?
            """,
            obj_id,
        )
        identity_map: Dict[str, Tuple[int, int]] = {row[0]: (int(row[1]), int(row[2])) for row in cur.fetchall()}

        # Default constraints per column (definition can be NVARCHAR(MAX))
        cur.execute(
            """
            SELECT c.name AS column_name,
                   CAST(dc.definition AS NVARCHAR(4000)) AS definition
            FROM sys.default_constraints AS dc
            JOIN sys.columns AS c ON dc.parent_object_id = c.object_id AND dc.parent_column_id = c.column_id
            WHERE dc.parent_object_id = ?
            """,
            obj_id,
        )
        default_map: Dict[str, str] = {row[0]: str(row[1]) for row in cur.fetchall()}

        # Computed columns definitions
        cur.execute(
            """
            SELECT c.name,
                   CAST(cc.definition AS NVARCHAR(4000)) AS definition,
                   cc.is_persisted
            FROM sys.computed_columns AS cc
            JOIN sys.columns AS c ON cc.object_id = c.object_id AND cc.column_id = c.column_id
            WHERE cc.object_id = ?
            """,
            obj_id,
        )
        computed_map: Dict[str, Tuple[str, bool]] = {row[0]: (str(row[1]), bool(row[2])) for row in cur.fetchall()}

        # Primary key (optional)
        cur.execute(
            """
            SELECT i.index_id, i.name, i.type_desc
            FROM sys.indexes AS i
            WHERE i.object_id = ? AND i.is_primary_key = 1
            """,
            obj_id,
        )
        pk_row = cur.fetchone()
        pk: Optional[Dict[str, object]] = None
        if pk_row:
            index_id, pk_name, type_desc = int(pk_row[0]), str(pk_row[1] or f"PK_{table_name}"), str(pk_row[2])
            cur.execute(
                """
                SELECT c.name, ic.is_descending_key
                FROM sys.index_columns AS ic
                JOIN sys.columns AS c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
                WHERE ic.object_id = ? AND ic.index_id = ?
                ORDER BY ic.key_ordinal
                """,
                obj_id,
                index_id,
            )
            pk_cols = [(str(r[0]), bool(r[1])) for r in cur.fetchall()]
            pk = {"name": pk_name, "type_desc": type_desc, "columns": pk_cols}

        tables.append(
            {
                "schema": schema_name,
                "name": table_name,
                "columns": columns,
                "identity_map": identity_map,
                "default_map": default_map,
                "computed_map": computed_map,
                "pk": pk,
            }
        )

    return tables

def generate_table_sql(rows: List[Dict[str, object]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name = str(r["schema"]), str(r["name"])
        columns: List[Dict[str, object]] = list(r["columns"])  # type: ignore
        identity_map: Dict[str, Tuple[int, int]] = dict(r["identity_map"])  # type: ignore
        default_map: Dict[str, str] = dict(r["default_map"])  # type: ignore
        computed_map: Dict[str, Tuple[str, bool]] = dict(r["computed_map"])  # type: ignore
        pk: Optional[Dict[str, object]] = r.get("pk")  # type: ignore

        col_defs: List[str] = []
        for c in columns:
            cname = str(c["name"])
            if bool(c.get("is_computed")) and cname in computed_map:
                expr, persisted = computed_map[cname]
                persisted_sql = " PERSISTED" if persisted else ""
                col_defs.append(f"{br(cname)} AS ({expr}){persisted_sql}")
                continue

            ctype = format_system_type(str(c["system_type"]), int(c["max_length"]), int(c["precision"]), int(c["scale"]))
            pieces: List[str] = [f"{br(cname)} {ctype}"]
            if cname in identity_map:
                seed, inc = identity_map[cname]
                pieces.append(f"IDENTITY({seed},{inc})")
            if c.get("collation"):
                pieces.append(f"COLLATE {c['collation']}")
            if cname in default_map:
                pieces.append(f"DEFAULT {default_map[cname]}")
            pieces.append("NULL" if bool(c["is_nullable"]) else "NOT NULL")
            col_defs.append(" ".join(pieces))

        # Primary key constraint
        if pk and pk.get("columns"):
            pk_name = br(str(pk["name"]))
            clustered = "CLUSTERED" if "CLUSTERED" in str(pk["type_desc"]).upper() else "NONCLUSTERED"
            col_list = ", ".join([f"{br(col)} {'DESC' if desc else 'ASC'}" for col, desc in pk["columns"]])  # type: ignore
            col_defs.append(f"CONSTRAINT {pk_name} PRIMARY KEY {clustered} ({col_list})")

        create_body = ",\n    ".join(col_defs)
        create_stmt = f"CREATE TABLE {qual(schema, name)} (\n    {create_body}\n);"
        block = (
            f"IF NOT EXISTS (SELECT 1 FROM sys.tables WHERE name = N'{tsql_quote(name)}' AND schema_id = SCHEMA_ID(N'{tsql_quote(schema)}'))\n"
            f"BEGIN\n    {create_stmt}\nEND\nGO\n"
        )
        blocks.append(block)
    return blocks


def fetch_modules(cur) -> List[Dict[str, object]]:
    """Fetch definitions of views, procedures, functions, and triggers."""
    cur.execute(
        """
        SELECT sch.name AS schema_name,
               o.name   AS object_name,
               o.type   AS object_type,
               CAST(m.definition AS VARBINARY(MAX)) AS definition_bin,
               m.uses_ansi_nulls,
               m.uses_quoted_identifier
        FROM sys.objects AS o
        JOIN sys.sql_modules AS m ON o.object_id = m.object_id
        JOIN sys.schemas AS sch ON o.schema_id = sch.schema_id
        WHERE o.is_ms_shipped = 0
          AND o.type IN ('V','P','FN','TF','IF','TR')
        ORDER BY sch.name, o.type, o.name
        """
    )
    rows = []
    for r in cur.fetchall():
        # definition is returned as VARBINARY(MAX); decode NVARCHAR(max) as UTF-16LE
        def_bin = r[3]
        try:
            if isinstance(def_bin, memoryview):
                def_bytes = def_bin.tobytes()
            elif isinstance(def_bin, (bytes, bytearray)):
                def_bytes = bytes(def_bin)
            else:
                # Fallback if driver already returned text
                def_text = str(def_bin)
                raise TypeError  # jump to except to use def_text
            def_text = def_bytes.decode("utf-16le", errors="replace")
        except Exception:
            # Best-effort fallback
            def_text = str(r[3])
        rows.append(
            {
                "schema": r[0],
                "name": r[1],
                "type": r[2],
                "definition": def_text,
                "ansi_nulls": bool(r[4]),
                "quoted_identifier": bool(r[5]),
            }
        )
    return rows


def generate_module_sql(rows: List[Dict[str, object]]) -> List[str]:
    blocks: List[str] = []
    for r in rows:
        schema, name, obj_type = str(r["schema"]), str(r["name"]), str(r["type"]).upper()
        ansi_on = "ON" if r["ansi_nulls"] else "OFF"
        qi_on = "ON" if r["quoted_identifier"] else "OFF"
        definition = normalize_create(str(r["definition"]).strip())

        # Ensure the module definition fully qualifies name with the same casing; we keep original text as-is.
        create_batch = f"SET ANSI_NULLS {ansi_on};\nSET QUOTED_IDENTIFIER {qi_on};\n{definition}"
        create_batch_escaped = tsql_quote(create_batch)

        # OBJECT_ID(name, type) works for all object types; using it keeps checks concise
        block = (
            f"IF OBJECT_ID(N'{tsql_quote(schema)}.{tsql_quote(name)}', '{obj_type}') IS NULL\n"
            f"    EXEC(N'{create_batch_escaped}');\nGO\n"
        )
        blocks.append(block)
    return blocks


# -------------------------
# Main Orchestration
# -------------------------


def main(argv: List[str]) -> int:
    print("Loading configuration from .env ...")
    cfg = load_config()

    print("Connecting to SQL Server ...")
    try:
        with connect(cfg) as conn:
            conn.timeout = int(cfg.get("CONNECTION_TIMEOUT", "30"))
            cur = conn.cursor()

            print("Fetching metadata ...")
            schemas = fetch_schemas(cur)
            alias_types = fetch_alias_types(cur)
            table_types = fetch_table_types(cur)
            sequences = fetch_sequences(cur)
            tables = fetch_tables(cur)
            synonyms = fetch_synonyms(cur)
            modules = fetch_modules(cur)

            print("Generating SQL script ...")
            out_path = os.path.join(os.getcwd(), "migration_script.sql")
            with open(out_path, "w", encoding="utf-8") as out:
                write_header(out)

                # Order: schemas -> types (alias, table) -> sequences -> tables -> synonyms -> modules
                for block in generate_schema_sql(schemas):
                    out.write(block + "\n")

                for block in generate_alias_type_sql(alias_types):
                    out.write(block + "\n")

                for block in generate_table_type_sql(table_types):
                    out.write(block + "\n")

                for block in generate_sequence_sql(sequences):
                    out.write(block + "\n")

                for block in generate_table_sql(tables):
                    out.write(block + "\n")

                for block in generate_synonym_sql(synonyms):
                    out.write(block + "\n")

                for block in generate_module_sql(modules):
                    out.write(block + "\n")

            print(f"Done. Wrote: {out_path}")
            return 0
    except pyodbc.Error as e:
        print("Database error:", e, file=sys.stderr)
        return 2
    except Exception as e:
        print("Error:", e, file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
